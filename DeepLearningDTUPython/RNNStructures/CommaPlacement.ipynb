{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence number 6:  106961\n",
      "54991 43211 33726 92320 53777 9075 87678 111230 65216 55675 54422 21582 39146 \n",
      "231 170 142 133 231 118 41 201 223 153 133 238 41 \n"
     ]
    }
   ],
   "source": [
    "#RNN from the weekly exercises.\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "#%matplotlib inline\n",
    "# %matplotlib nbagg\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from DataGenerationCommaPlacementWithWordTypes import get_batch_comma\n",
    "from DataGenerationCommaPlacementWithWordTypes import num_of_training_samples\n",
    "from DataGenerationCommaPlacementWithWordTypes import get_word_stem_from_encoding\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join('.', '..')) \n",
    "#from tempUtils import utils\n",
    "\n",
    "import tf_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load previous model\n",
    "At the bottom of the script there is some code which saves the model.  \n",
    "If you wish to restore your model from a previous state use this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_model = True \n",
    "#model_path = \"/tmp/RNN/modelRNNOnlyTypes64.ckpt-530\"\n",
    "model_path = \"tmp/modelRNNBig.ckpt-385\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputdata\n",
    "Outputs two samples where:   \n",
    "ENCODED INPUTS is a vector with reference to a word in the dict word_stems.  \n",
    "ENCODED INPUT TYPES is a vector with reference to the word type in the dict word_types.  \n",
    "ENCODED TARGETS OUTPUT is the target vector, where 1 is commas and 0 is everything else.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (2, 20)\n",
      "input types: int32\n",
      "\n",
      "SAMPLE 0\n",
      "ENCODED INPUTS:\t\t\t [ 86374  94286  43211  51222  95495  96390  41472 111230  62764  70687\n",
      "   4546 115045  71154  80217  98112  53845  12586 115045  97690  39146]\n",
      "ENCODED INPUT TYPES: \t\t [217 217 170 251 268 268 238 201 170 268 238 133 217 153 268 208 217 133\n",
      " 238  41]\n",
      "INPUTS SEQUENCE LENGTH:\t\t 20\n",
      "TEXT TARGETS OUTPUT:\t\t 13#\n",
      "TEXT TARGETS INPUT:\t\t #86374 94286 43211 51222 95495 96390 41472 111230 62764 70687 4546 115045 71154 87678 80217 98112 53845 12586 115045 97690 39146 \n",
      "ENCODED TARGETS OUTPUT:\t\t [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "ENCODED TARGETS INPUT:\t\t [ 86374  94286  43211  51222  95495  96390  41472 111230  62764  70687\n",
      "   4546 115045  71154  80217  98112  53845  12586 115045  97690  39146]\n",
      "INPUT TARGETS SEQUENCE LENGTH:\t 20\n",
      "OUTPUT TARGETS SEQUENCE LENGTH:\t 20\n",
      "TARGETS MASK:\t\t\t [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.]\n",
      "\n",
      "SAMPLE 1\n",
      "ENCODED INPUTS:\t\t\t [ 83324 116978 117583  82187  91339  70077  16981  95702  74861  36632\n",
      "  72937  54775  15631  23130  46760      0      0      0      0      0]\n",
      "ENCODED INPUT TYPES: \t\t [ 78 170 268 153 238 247 133 153 247 166 268 238  41 217 217   0   0   0\n",
      "   0   0]\n",
      "INPUTS SEQUENCE LENGTH:\t\t 15\n",
      "TEXT TARGETS OUTPUT:\t\t 4#\n",
      "TEXT TARGETS INPUT:\t\t #83324 116978 117583 82187 87678 91339 70077 16981 95702 74861 36632 72937 54775 15631 23130 46760 \n",
      "ENCODED TARGETS OUTPUT:\t\t [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "ENCODED TARGETS INPUT:\t\t [ 83324 116978 117583  82187  91339  70077  16981  95702  74861  36632\n",
      "  72937  54775  15631  23130  46760      0      0      0      0      0]\n",
      "INPUT TARGETS SEQUENCE LENGTH:\t 15\n",
      "OUTPUT TARGETS SEQUENCE LENGTH:\t 15\n",
      "TARGETS MASK:\t\t\t [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.\n",
      "  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "#inputs, inputs_seqlen, targets_in, targets_out, targets_seqlen, targets_mask, \\\n",
    "#text_inputs, text_targets_in, text_targets_out = \\\n",
    "#        get_batch(batch_size=batch_size, max_digits=10, min_digits=5)\n",
    "\n",
    "inputs, inputs_word_types, inputs_seqlen, targets_in, targets_out, targets_in_seqlen, targets_out_seqlen, targets_mask, \\\n",
    "text_targets_in, text_targets_out = \\\n",
    "        get_batch_comma(batch_size=batch_size, indices_of_interest = [0,1])\n",
    "    \n",
    "    \n",
    "print(\"input shape:\", inputs.shape)\n",
    "print(\"input types:\", inputs.dtype)#, inputs_seqlen.dtype, targets_in.dtype, targets_out.dtype, targets_seqlen.dtype)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    print(\"\\nSAMPLE\",i)\n",
    "    print(\"ENCODED INPUTS:\\t\\t\\t\", inputs[i])\n",
    "    print(\"ENCODED INPUT TYPES: \\t\\t\", inputs_word_types[i])\n",
    "    print(\"INPUTS SEQUENCE LENGTH:\\t\\t\", inputs_seqlen[i])\n",
    "    print(\"TEXT TARGETS OUTPUT:\\t\\t\", text_targets_out[i])\n",
    "    print(\"TEXT TARGETS INPUT:\\t\\t\", text_targets_in[i])\n",
    "    print(\"ENCODED TARGETS OUTPUT:\\t\\t\", targets_out[i])\n",
    "    print(\"ENCODED TARGETS INPUT:\\t\\t\", targets_in[i])\n",
    "    print(\"INPUT TARGETS SEQUENCE LENGTH:\\t\", targets_in_seqlen[i])\n",
    "    print(\"OUTPUT TARGETS SEQUENCE LENGTH:\\t\", targets_out_seqlen[i])\n",
    "    print(\"TARGETS MASK:\\t\\t\\t\", targets_mask[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input embedded shape: (?, ?, 16)\n"
     ]
    }
   ],
   "source": [
    "# resetting the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Setting up hyperparameters and general configs\n",
    "NUM_INPUTS = 133000\n",
    "NUM_INPUT_TYPES = 317 #a lot of junk is contained there\n",
    "NUM_OUTPUTS = 2 #0,1,2 #(0-19 +\",\")\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# try various learning rates 1e-2 to 1e-5\n",
    "LEARNING_RATE = 0.0005 #0.005\n",
    "X_EMBEDDINGS = 8\n",
    "X_TYPES_EMBEDDINGS = 8\n",
    "t_EMBEDDINGS = 8\n",
    "NUM_UNITS_ENC = 256 #512\n",
    "NUM_UNITS_DEC = 256 #512\n",
    "number_of_layers = 1 #3\n",
    "\n",
    "# Setting up placeholders, these are the tensors that we \"feed\" to our network\n",
    "Xs = tf.placeholder(tf.int32, shape=[None, None], name='X_input')\n",
    "Xs_types = tf.placeholder(tf.int32, shape=[None, None], name='X_input_types')\n",
    "ts_in = tf.placeholder(tf.int32, shape=[None, None], name='t_input_in')\n",
    "ts_out = tf.placeholder(tf.int32, shape=[None, None], name='t_input_out')\n",
    "X_len = tf.placeholder(tf.int32, shape=[None], name='X_len')\n",
    "t_in_len = tf.placeholder(tf.int32, shape=[None], name='t_in_len')\n",
    "t_out_len = tf.placeholder(tf.int32, shape=[None], name='t_out_len')\n",
    "t_mask = tf.placeholder(tf.float32, shape=[None, None], name='t_mask')\n",
    "drop = tf.placeholder_with_default(0.0,shape =())\n",
    "\n",
    "### Building the model\n",
    "# first we build the embeddings to make our characters into dense, trainable vectors\n",
    "X_embeddings = tf.get_variable('X_embeddings', [NUM_INPUTS, X_EMBEDDINGS],\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "X_TYPES_embeddings = tf.get_variable('X_TYPES_embeddings', [NUM_INPUT_TYPES, X_TYPES_EMBEDDINGS],\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "t_embeddings = tf.get_variable('t_embeddings', [NUM_INPUTS, t_EMBEDDINGS],\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "X_embedded = tf.gather(X_embeddings, Xs, name='embed_X')\n",
    "X_TYPES_embedded = tf.gather(X_TYPES_embeddings, Xs_types, name='embed_X_types')\n",
    "t_embedded = tf.gather(t_embeddings, ts_in, name='embed_t')\n",
    "input_embedded = tf.concat([X_embedded, X_TYPES_embedded],2)\n",
    "\n",
    "## forward encoding - use deep cell, with multiple GRU cells stacked on top of each other.\n",
    "\n",
    "print(\"input embedded shape:\", input_embedded.shape)\n",
    "def gru_cell():\n",
    "    return tf.nn.rnn_cell.GRUCell(NUM_UNITS_ENC)\n",
    "    #return tf.nn.rnn_cell.DropoutWrapper(cell,input_size=8,input_keep_prob=1.0-drop, state_keep_prob=1.0, output_keep_prob=1.0,dtype=tf.float32,\n",
    "    #                                     variational_recurrent=False)\n",
    "    #return tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=1.0-drop, dtype=tf.float32, variational_recurrent=True)\n",
    "stacked_gru = tf.contrib.rnn.MultiRNNCell(\n",
    "    [gru_cell() for _ in range(number_of_layers)])\n",
    "\n",
    "\n",
    "\n",
    "enc_out, enc_state = tf.nn.dynamic_rnn(cell=stacked_gru, inputs=input_embedded,\n",
    "                                  sequence_length=X_len, dtype=tf.float32)\n",
    "# dynamic Rnn of a stack, outputs the final state of each layer. We are only interested in the final state of the final layer\n",
    "enc_state = enc_state[-1]\n",
    "\n",
    "# use below incase TF's makes issues\n",
    "# enc_state, _ = tf_utils.encoder(X_embedded, X_len, 'encoder', NUM_UNITS_ENC)\n",
    "#\n",
    "# enc_state = tf.concat(1, [enc_state, enc_state])\n",
    "\n",
    "## decoding\n",
    "# note that we are using a wrapper for decoding here, this wrapper is hardcoded to only use GRU\n",
    "# check out tf_utils to see how you make your own decoder\n",
    "\n",
    "# setting up weights for computing the final output\n",
    "W_out = tf.get_variable('W_out', [NUM_UNITS_DEC, NUM_OUTPUTS])\n",
    "b_out = tf.get_variable('b_out', [NUM_OUTPUTS])\n",
    "\n",
    "dec_out, valid_dec_out = tf_utils.decoder(enc_state, t_embedded, tf.shape(t_embedded)[1],\n",
    "                                          NUM_UNITS_DEC, t_embeddings,\n",
    "                                          W_out, b_out)\n",
    "\n",
    "#This code extracts the output of just the last rnn cell in the sequence. Enc_out has the format \n",
    "#(Batch_size, max_sequence_length, final state)\n",
    "#out_tensor = tf.slice(enc_out, [0, 0 ,0],[-1,1,-1])\n",
    "#valid_out_tensor = tf.slice(enc_out,[0,0,0],[-1,1,-1])\n",
    "\n",
    "## reshaping to have [batch_size*seqlen, num_units]\n",
    "out_tensor = tf.reshape(dec_out, [-1, NUM_UNITS_DEC])\n",
    "valid_out_tensor = tf.reshape(valid_dec_out, [-1, NUM_UNITS_DEC])\n",
    "\n",
    "# computing output\n",
    "out_tensor = tf.matmul(out_tensor, W_out) + b_out\n",
    "valid_out_tensor = tf.matmul(valid_out_tensor, W_out) + b_out\n",
    "\n",
    "#out_tensor = tf.expand_dims(out_tensor, 1)\n",
    "#valid_out_tensor = tf.expand_dims(valid_out_tensor, 1)\n",
    "\n",
    "## reshaping back to sequence\n",
    "# print('X_len', tf.shape(X_len)[0])\n",
    "b_size = tf.shape(X_len)[0] # use a variable we know has batch_size in [0]\n",
    "seq_length = tf.shape(t_embedded)[1] # variable we know has sequence length in [1]\n",
    "num_out = tf.constant(NUM_OUTPUTS) # casting NUM_OUTPUTS to a tensor variable\n",
    "out_shape = tf.concat([tf.expand_dims(b_size, 0),\n",
    "                      tf.expand_dims(seq_length,0),\n",
    "                      tf.expand_dims(num_out, 0)],\n",
    "                     axis=0)\n",
    "outShapeForPrint = tf.Print(out_shape, [out_shape],message= \"this is output shape\")\n",
    "out_tensor = tf.reshape(out_tensor, out_shape)\n",
    "\n",
    "valid_out_tensor = tf.reshape(valid_out_tensor, out_shape)\n",
    "# handling shape loss\n",
    "#out_tensor.set_shape([None, None, NUM_OUTPUTS])\n",
    "y = out_tensor\n",
    "y_valid = valid_out_tensor\n",
    "\n",
    "# print all the variable names and shapes\n",
    "for var in tf.global_variables ():\n",
    "    s = var.name + \" \"*(40-len(var.name))\n",
    "    #print(s, var.value().get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining loss function, cost functions etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def loss_and_acc(preds):\n",
    "    # sequence_loss_tensor is a modification of TensorFlow's own sequence_to_sequence_loss\n",
    "    # TensorFlow's seq2seq loss works with a 2D list instead of a 3D tensors\n",
    "    loss = tf_utils.sequence_loss_tensor(preds, ts_out, t_mask, NUM_OUTPUTS) # notice that we use ts_out here!\n",
    "\n",
    "    # if you want regularization\n",
    "    reg_scale = 0.00001\n",
    "    regularize = tf.contrib.layers.l2_regularizer(reg_scale)\n",
    "    params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    reg_term = sum([regularize(param) for param in params])\n",
    "    loss += reg_term\n",
    "    \n",
    "    ## calculate accuracy\n",
    "    argmax = tf.to_int32(tf.argmax(preds, 2))\n",
    "    correct = tf.to_float(tf.equal(argmax, ts_out)) * t_mask\n",
    "    accuracy = tf.reduce_sum(correct) / tf.reduce_sum(t_mask)\n",
    "\n",
    "## Calculate only relevant part of accuracy\n",
    "    correct1 = tf.reduce_sum(correct,1)-tf.reduce_sum(t_mask, 1)   \n",
    "    correct2 = tf.clip_by_value(correct1, -1.0,1.0)\n",
    "    #correct25 = tf.count_nonzero(correct2, dtype=tf.float32)\n",
    "    correct3 = tf.subtract(1.0,tf.divide(tf.count_nonzero(correct2, dtype=tf.float32), tf.cast(tf.shape(correct)[0],tf.float32)))\n",
    "    \n",
    "    return loss, accuracy, argmax, correct3\n",
    "\n",
    "loss, accuracy, predictions, cor1 = loss_and_acc(y)\n",
    "loss_valid, accuracy_valid, predictions_valid, cor1_val = loss_and_acc(y_valid)\n",
    "\n",
    "# use lobal step to keep track of our iterations\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# pick optimizer, try momentum or adadelta epsilon has to be higher than the standard, as it will otherwise cause the\n",
    "#optimizer to go crazy when close to the global optimum.\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE, epsilon= 0.001)\n",
    "\n",
    "# extract gradients for each variable\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "\n",
    "## add below for clipping by norm\n",
    "# gradients, variables = zip(*grads_and_vars)  # unzip list of tuples\n",
    "# clipped_gradients, global_norm = (\n",
    "#    tf.clip_by_global_norm(gradients, self.clip_norm) )\n",
    "# grads_and_vars = zip(clipped_gradients, variables)\n",
    "# apply gradients and make trainable function\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "# print all the variable names and shapes\n",
    "# notice that we now have the optimizer Adam as well!\n",
    "for var in tf.global_variables():\n",
    "    s = var.name + \" \"*(40-len(var.name))\n",
    "    #print(s, var.value().get_shape())\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/modelRNNBig.ckpt-385\n",
      "INFO:tensorflow:Restoring parameters from tmp/modelRNNBig.ckpt-385\n",
      "\n",
      "SAMPLE 0\n",
      "\n",
      "SAMPLE 1\n",
      "y (2, 20, 2)\n",
      "X_embedded (2, 20, 8)\n",
      "input_types_embedded (2, 20, 16)\n",
      "y_valid (2, 20, 2)\n",
      "number of batches 378\n"
     ]
    }
   ],
   "source": [
    "## Start the session\n",
    "# restricting memory usage, TensorFlow is greedy and will use all memory otherwise\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.35)#0.35\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts))#,inter_op_parallelism_threads=4, intra_op_parallelism_threads=4))\n",
    "\n",
    "# Initialize parameters\n",
    "if load_model:\n",
    "    try:\n",
    "        print(model_path)\n",
    "        tf.train.Saver().restore(sess, model_path)\n",
    "    except:\n",
    "        print(\"this is the error: \")\n",
    "        print(sys.exc_info()[1])\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Model not found, new parameters initialized')\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# as always, test the forward pass and initialize the tf.Session!\n",
    "for i in range(batch_size):\n",
    "    print(\"\\nSAMPLE\",i)\n",
    "   # print(\"TEXT INPUTS:\\t\\t\\t\", text_inputs[i])\n",
    "    #print(\"TEXT TARGETS INPUT:\\t\\t\", text_targets_in[i])\n",
    "\n",
    "feed_dict = {Xs: inputs, Xs_types: inputs_word_types, X_len: inputs_seqlen, ts_in: targets_in,\n",
    "             ts_out: targets_out, t_in_len: targets_in_seqlen, t_out_len: targets_out_seqlen}\n",
    "\n",
    "# test training forwardpass\n",
    "#print(\"targets seqlen\", targets_in_seqlen)\n",
    "#print(\"targets out seqlen\", targets_out_seqlen)\n",
    "fetches = [y, X_embedded, enc_state, input_embedded]\n",
    "#fetches = [dec_out]\n",
    "res = sess.run(fetches=fetches, feed_dict=feed_dict)\n",
    "#print(\"out shape:\", res[0].shape)\n",
    "#print(\"inputs\", inputs)\n",
    "#print(\"input length\", inputs_seqlen)\n",
    "#print(\"encoder state\", res[2].shape)\n",
    "#print(\"targets output\", targets_out)\n",
    "\n",
    "print(\"y\", res[0].shape)\n",
    "print(\"X_embedded\", res[1].shape)\n",
    "print(\"input_types_embedded\", res[3].shape )\n",
    "\n",
    "# test validation forwardpass\n",
    "fetches = [y_valid]\n",
    "res = sess.run(fetches=fetches, feed_dict=feed_dict)\n",
    "print(\"y_valid\", res[0].shape)\n",
    "\n",
    "#Some Data hyperparameters\n",
    "dropout_perc = 0.7\n",
    "num_epochs = 2\n",
    "num_of_training_samples_loaded = num_of_training_samples()\n",
    "num_of_samples_for_validation = 10000\n",
    "num_of_training_samples_minus_validation = num_of_training_samples_loaded - num_of_samples_for_validation\n",
    "num_batches_train = int(num_of_training_samples_minus_validation / BATCH_SIZE)\n",
    "print(\"number of batches\", num_batches_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "X_val (10000, 27)\n",
      "t_out_val (10000, 27)\n"
     ]
    }
   ],
   "source": [
    "#Generate some validation data\n",
    "validation_indices = list(range(num_of_training_samples_minus_validation, num_of_training_samples_minus_validation+num_of_samples_for_validation))\n",
    "print(len(validation_indices))\n",
    "X_val, X_types_val, X_len_val, t_in_val, t_out_val, t_len_in_val, t_len_out_val,t_mask_val, \\\n",
    " text_targets_in_val, text_targets_out_val = \\\n",
    "     get_batch_comma(batch_size=num_of_samples_for_validation, indices_of_interest = validation_indices)\n",
    "print(\"X_val\", X_val.shape)\n",
    "print(\"t_out_val\", t_out_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "## If you get an error, remove this line! It makes the error message hard to understand.\n",
    "\n",
    "# setting up running parameters\n",
    "val_interval = 500\n",
    "saver = tf.train.Saver()\n",
    "samples_val = []\n",
    "epochs, costs, accs_val = [], [], []\n",
    "plt.figure()\n",
    "train_acc = []\n",
    "corr_train = []\n",
    "accs_val = []\n",
    "batch_number = 0\n",
    "\n",
    "if load_model == False:\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            loss_train = 0\n",
    "            acc_train = 0\n",
    "            cor_train = 0\n",
    "            percentage_validation_set_to_evaluate = 2 #20 percent\n",
    "            print(\"Epoch: \", epoch)\n",
    "            if epoch % 5 == 0:\n",
    "                saver.save(sess,model_path, global_step=epoch)\n",
    "            for i in range(num_batches_train):\n",
    "                # load data\n",
    "                # print(\"Epoch {0} new Batch: {1} \".format(epoch, i))\n",
    "\n",
    "                # Select random part of the training data\n",
    "                idxs = np.random.choice(range(num_of_training_samples_minus_validation), size=(BATCH_SIZE), replace=False)\n",
    "                X_tr, X_types_tr, X_len_tr, t_in_tr, t_out_tr, t_len_in_tr, t_len_out_tr, t_mask_tr, \\\n",
    "                text_targets_in_tr, text_targets_out_tr = \\\n",
    "                    get_batch_comma(batch_size=BATCH_SIZE, indices_of_interest=idxs)\n",
    "                # make fetches\n",
    "                fetches_tr = [train_op, loss, accuracy, cor1]\n",
    "                # set up feed dict\n",
    "                feed_dict_tr = {Xs: X_tr, Xs_types: X_types_tr, X_len: X_len_tr, ts_in: t_in_tr,\n",
    "                                ts_out: t_out_tr, t_in_len: t_len_in_tr, t_out_len: t_len_out_tr, t_mask: t_mask_tr, drop: dropout_perc}\n",
    "                # run the model\n",
    "                res = tuple(sess.run(fetches=fetches_tr, feed_dict=feed_dict_tr))\n",
    "                _, batch_cost, batch_acc, batch_cor = res\n",
    "                loss_train += batch_cost\n",
    "                acc_train += batch_acc\n",
    "                cor_train += batch_cor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Validation part:\n",
    "            print(\"validating\")\n",
    "            if epoch %10 == 0:\n",
    "                percentage_validation_set_to_evaluate = 10 #100 percent\n",
    "            total_acc_val = 0\n",
    "            cor_val = 0\n",
    "            for val_part in range(percentage_validation_set_to_evaluate):\n",
    "                lowerLimit = val_part * 1000\n",
    "                upperLimit = (val_part+1) *1000\n",
    "                fetches_val = [accuracy_valid, y_valid, cor1_val]\n",
    "                feed_dict_val = {Xs: X_val[lowerLimit:upperLimit], Xs_types: X_types_val[lowerLimit:upperLimit],\n",
    "                                 X_len: X_len_val[lowerLimit:upperLimit], ts_in: t_in_val[lowerLimit:upperLimit],\n",
    "                                 ts_out: t_out_val[lowerLimit:upperLimit], t_in_len: t_len_in_val[lowerLimit:upperLimit], t_out_len: t_len_out_val[lowerLimit:upperLimit],\n",
    "                                 t_mask: t_mask_val[lowerLimit:upperLimit]}\n",
    "                res = tuple(sess.run(fetches=fetches_val, feed_dict=feed_dict_val))\n",
    "                acc_val, output_val, cor = res\n",
    "                total_acc_val = total_acc_val+acc_val\n",
    "                cor_val = cor_val + cor\n",
    "            total_acc_val = total_acc_val / percentage_validation_set_to_evaluate\n",
    "            cor_val = cor_val / percentage_validation_set_to_evaluate\n",
    "\n",
    "            epochs += [epoch]\n",
    "            accs_val += [cor_val]\n",
    "            print(\"Epochs: \", epochs)\n",
    "            print(\"accs_val: \", accs_val)\n",
    "            print(\"correct_validation_sentences: \", cor_val)\n",
    "            plt.figure(1)\n",
    "            plt.plot(epochs, accs_val, 'g-')\n",
    "            plt.ylabel('Validation Accuracy', fontsize=15)\n",
    "            plt.xlabel('Epoch', fontsize=15)\n",
    "            plt.title('', fontsize=20)\n",
    "            plt.grid('on')\n",
    "            plt.savefig(\"out.png\")\n",
    "            display.display(display.Image(filename=\"out.png\"))\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "            #training evaluation\n",
    "            costs += [loss_train]\n",
    "            train_acc += [acc_train / num_batches_train]\n",
    "            corr_train += [cor_train / num_batches_train]\n",
    "\n",
    "            print(\"Costs: \", costs)\n",
    "            print(\"train Acc:\", train_acc)\n",
    "            print(\"# of Correct training sentences: \", corr_train)\n",
    "\n",
    "            plt.figure(1)\n",
    "            #plt.legend('training Acc')\n",
    "            plt.ylabel('Acc')\n",
    "            plt.plot(epochs, corr_train, color=\"red\")\n",
    "            # plt.plot(updates, KL_valid, color=\"blue\", linestyle=\"--\")\n",
    "            plt.ticklabel_format(style='sci', axis='x', scilimits=(0, 0))\n",
    "            plt.grid('on')\n",
    "            plt.savefig(\"outTrainAcc.png\")\n",
    "\n",
    "            # plt.subplot(num_classes + 2, 2, 3)\n",
    "            plt.figure(2)\n",
    "            plt.legend('training loss')\n",
    "            plt.ylabel('cost')\n",
    "            plt.plot(epochs, costs, color=\"green\")\n",
    "            # plt.plot(updates, KL_valid, color=\"blue\", linestyle=\"--\")\n",
    "            plt.ticklabel_format(style='sci', axis='x', scilimits=(0, 0))\n",
    "            plt.grid('on')\n",
    "            plt.savefig(\"outLoss.png\")\n",
    "\n",
    "            # plt.subplot(num_classes + 2, 2, 3)\n",
    "\n",
    "            #evaluate the model\n",
    "            if epoch % 5 == 0:\n",
    "                # test the model:\n",
    "                passed = 0\n",
    "                failed = 0\n",
    "                offset = 1000\n",
    "                numberOfCases = 1000\n",
    "                # for i in range(1, 10):\n",
    "\n",
    "                X_tr, X_types_tr, X_len_tr, t_in_tr, t_out_tr, t_len_tr, t_out_len_tr, t_mask_tr, \\\n",
    "                text_targets_in_tr, text_targets_out_tr = \\\n",
    "                    get_batch_comma(batch_size=numberOfCases,\n",
    "                                    indices_of_interest=validation_indices[offset:numberOfCases + offset])\n",
    "                # make fetches\n",
    "                fetches_check = [y, cor1]\n",
    "                # set up feed dict\n",
    "                feed_dict_check = {Xs: X_tr, Xs_types: X_types_tr, X_len: X_len_tr, ts_in: t_in_tr,\n",
    "                                ts_out: t_out_tr, t_in_len: t_len_tr, t_out_len: t_out_len_tr, t_mask: t_mask_tr}\n",
    "                # run the model\n",
    "                res = tuple(sess.run(fetches=fetches_check, feed_dict=feed_dict_check))\n",
    "                for j in range(0, numberOfCases):\n",
    "                    # print(\"input\", text_inputs_tr)\n",
    "\n",
    "\n",
    "                    # print(\"input\", text_targets_in_tr)\n",
    "                    # print(\"y: \", np.argmax(res[3], axis=2))\n",
    "\n",
    "                    # convert input back to text:\n",
    "                    outputStr = \"\"\n",
    "\n",
    "                    for k in range(0, X_len_tr[j]):\n",
    "                        if t_out_tr[j][k] == 1:\n",
    "                            outputStr += \",\"\n",
    "\n",
    "                        outputStr += \" \"\n",
    "                        outputStr += get_word_stem_from_encoding(X_tr[j][k])\n",
    "\n",
    "                    relevant_part_of_facit = np.logical_and(t_out_tr[j], t_mask_tr[j]).astype(int)\n",
    "                    relevant_part_of_prediction = np.logical_and(np.argmax(res[0][j], axis=1), t_mask_tr[j]).astype(int)\n",
    "                    np.set_printoptions(suppress=True)\n",
    "                    if (relevant_part_of_facit - relevant_part_of_prediction).any():\n",
    "                        print(\"fail! index\", j)\n",
    "                        print(\"fail! target: \", t_out_tr[j])\n",
    "                        print(\"fail! output: \", 1.0/(1+np.exp(-res[0][j])))\n",
    "                        print(\"fail! y argmax: \", np.argmax(res[0][j], axis=1))\n",
    "                        failed = failed + 1\n",
    "                        print(outputStr)\n",
    "                        break\n",
    "                print(\"Failed: \", (failed + 0.0) / numberOfCases)\n",
    "                print(\"Correct: \", res[1])\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #plot of validation accuracy for each target position\n",
    "\n",
    "if load_model == False:\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.plot(np.mean(np.argmax(output_val,axis=2)==t_out_val,axis=0))\n",
    "    plt.ylabel('Accuracy', fontsize=15)\n",
    "    plt.xlabel('Target position', fontsize=15)\n",
    "    plt.title('', fontsize=20)\n",
    "    plt.grid('on')\n",
    "    plt.show()\n",
    "#why do the plot look like this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: 0.96\n",
      "fail! index 7\n",
      "fail! target:  [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "fail! output: \n",
      " [[ 0.99999952  0.00000057]\n",
      " [ 0.99993062  0.00008572]\n",
      " [ 0.99995887  0.0000478 ]\n",
      " [ 0.99999762  0.00000268]\n",
      " [ 0.85893363  0.16132665]\n",
      " [ 0.94691497  0.06215469]\n",
      " [ 0.98987854  0.01318684]\n",
      " [ 0.99999523  0.00000687]\n",
      " [ 0.9688468   0.05816472]\n",
      " [ 0.99982941  0.00033736]\n",
      " [ 0.96718496  0.06945771]\n",
      " [ 0.92915642  0.145089  ]\n",
      " [ 1.          0.00000003]\n",
      " [ 0.99987543  0.0002828 ]\n",
      " [ 0.99999952  0.00000103]\n",
      " [ 0.99988234  0.00025577]\n",
      " [ 0.99999082  0.0000192 ]\n",
      " [ 1.          0.00000008]\n",
      " [ 0.99285853  0.01563418]\n",
      " [ 0.9999938   0.0000121 ]\n",
      " [ 0.99999845  0.00000293]\n",
      " [ 0.99801421  0.0041428 ]]\n",
      "input #88137 15784 51222 117583 87678 22321 112872 115045 17175 54422 2247 107841 53845 84428 15631 115045 1321 26419 46760 56798 125725 39146 \n",
      "Sentence:  Der danne en stor, endestil blomsterstand med klase af hunblomst nederst og hanblomst ( med fem støvbærer ) i spids .\n",
      "relevant part of prediction  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "fail! index 8\n",
      "fail! target:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "y:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "fail! output: \n",
      " [[ 0.99999988  0.00000011]\n",
      " [ 0.99970764  0.00037245]\n",
      " [ 0.99999964  0.00000034]\n",
      " [ 1.          0.00000002]\n",
      " [ 0.99851233  0.00172606]\n",
      " [ 0.99999738  0.00000296]\n",
      " [ 0.79833704  0.23614141]\n",
      " [ 0.99960691  0.00049062]\n",
      " [ 0.98907787  0.01750093]\n",
      " [ 1.          0.00000001]\n",
      " [ 0.99999976  0.00000044]\n",
      " [ 0.99993908  0.00009146]\n",
      " [ 0.99998188  0.00002765]\n",
      " [ 0.96975809  0.05094351]\n",
      " [ 0.99949539  0.00085582]\n",
      " [ 0.62891716  0.51571798]\n",
      " [ 0.99693668  0.00549271]\n",
      " [ 0.99999487  0.00000915]\n",
      " [ 0.99872655  0.00258671]\n",
      " [ 0.99997938  0.0000412 ]\n",
      " [ 0.99997199  0.00005565]\n",
      " [ 0.99995577  0.00008509]]\n",
      "input #27968 43211 51222 21488 115045 41821 43424 123087 53845 51222 6578 19855 46108 115045 117583 87678 105038 124427 39146 \n",
      "Sentence:  Frugt være en bær med læderagtig ydre væg og en blød indre masse med stor, sort frø .\n",
      "relevant part of prediction  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "fail! index 39\n",
      "fail! target:  [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "fail! output: \n",
      " [[ 0.99999952  0.00000083]\n",
      " [ 0.65270025  0.47433582]\n",
      " [ 0.99922192  0.0011292 ]\n",
      " [ 0.99999988  0.00000013]\n",
      " [ 0.99954766  0.00078565]\n",
      " [ 0.83764523  0.25713015]\n",
      " [ 0.97712463  0.04215131]\n",
      " [ 0.98792636  0.02434627]\n",
      " [ 0.94446939  0.1132314 ]\n",
      " [ 0.9999398   0.00012653]\n",
      " [ 0.9999541   0.00009168]\n",
      " [ 0.99997473  0.00005115]\n",
      " [ 0.99998915  0.00002078]\n",
      " [ 0.99999738  0.00000459]\n",
      " [ 0.99954045  0.00075105]\n",
      " [ 0.82571948  0.25140572]\n",
      " [ 0.99995232  0.00007469]\n",
      " [ 0.96317822  0.06516757]\n",
      " [ 0.99995971  0.00007857]\n",
      " [ 0.99992549  0.00015366]\n",
      " [ 0.99981254  0.00039203]\n",
      " [ 0.9998247   0.00035324]]\n",
      "input #97396 34008 56798 113372 51533 36023 125911 36023 87678 36632 28609 108042 54422 36632 38132 98009 3898 39146 \n",
      "Sentence:  corsis indlæg i Jacopo Peris ¤ Daf ¤, den eneste bevare af den gammel kende opera .\n",
      "relevant part of prediction  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "fail! index 54\n",
      "fail! target:  [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "y:  [0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "fail! output: \n",
      " [[ 0.99999845  0.0000019 ]\n",
      " [ 0.99998522  0.00001572]\n",
      " [ 0.99989271  0.0000899 ]\n",
      " [ 0.99990022  0.00006997]\n",
      " [ 0.99972183  0.00017958]\n",
      " [ 0.9999572   0.00002775]\n",
      " [ 0.99994373  0.00004321]\n",
      " [ 0.99932361  0.00060023]\n",
      " [ 0.50485533  0.52060181]\n",
      " [ 0.99293363  0.00974518]\n",
      " [ 0.99998069  0.00002751]\n",
      " [ 0.98848826  0.02150751]\n",
      " [ 0.99955505  0.00090142]\n",
      " [ 0.19879165  0.91755199]\n",
      " [ 0.99999917  0.00000188]\n",
      " [ 0.99753928  0.00658307]\n",
      " [ 0.99977261  0.00059399]\n",
      " [ 0.99886203  0.00328254]\n",
      " [ 0.99996376  0.00010984]\n",
      " [ 0.99999845  0.00000504]\n",
      " [ 0.9994666   0.00172909]\n",
      " [ 0.99996293  0.00011335]]\n",
      "input #73624 107690 113653 51222 112168 28741 54422 72574 71275 115370 64726 50088 112987 87678 53845 82970 82948 127920 17220 75298 39146 \n",
      "Sentence:  H. disputats give en kritisk samling af al hvad man vide om Arkimedes, og udrede denne værk indbyrdes kronologi .\n",
      "relevant part of prediction  [0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Failed:  0.04\n"
     ]
    }
   ],
   "source": [
    "# ## Save model\n",
    "# # Read more about saving and loading models at https://www.tensorflow.org/programmers_guide/saved_model\n",
    "#\n",
    "# # Save model\n",
    "if load_model == False:\n",
    "    save_path = tf.train.Saver().save(sess, model_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "def decode_input_to_string(sequence_length, encoded_input, encoded_input_with_comma):\n",
    "    # convert input back to text:\n",
    "    outputStr = \"\"\n",
    "    for i in range(0, sequence_length):\n",
    "        if encoded_input_with_comma[i] == 1:\n",
    "            outputStr += \",\"\n",
    "\n",
    "        outputStr += \" \"\n",
    "        outputStr += get_word_stem_from_encoding(encoded_input[i])\n",
    "    return outputStr\n",
    "\n",
    "# test the model:\n",
    "#for i in range (1,10):\n",
    "passed = 0\n",
    "failed = 0\n",
    "countt = 0\n",
    "offset = 0\n",
    "numberOfCases = 100\n",
    "#for i in validation_indices:\n",
    "for i in range (1,2):\n",
    "    X_tr, X_types_tr, X_len_tr, t_in_tr, t_out_tr, t_len_tr, t_out_len_tr, t_mask_tr, \\\n",
    "             text_targets_in_tr, text_targets_out_tr = \\\n",
    "                get_batch_comma(batch_size=numberOfCases,indices_of_interest = validation_indices[offset:numberOfCases+offset])\n",
    "    # make fetches\n",
    "    fetches_tr = [train_op, loss, accuracy, y, cor1]\n",
    "    # set up feed dict\n",
    "    feed_dict_tr = {Xs: X_tr, Xs_types: X_types_tr, X_len: X_len_tr, ts_in: t_in_tr,\n",
    "             ts_out: t_out_tr, t_in_len: t_len_tr, t_out_len: t_out_len_tr,t_mask: t_mask_tr}\n",
    "    # run the model\n",
    "    res = tuple(sess.run(fetches=fetches_tr, feed_dict=feed_dict_tr))\n",
    "    #print(\"input\", text_inputs_tr)\n",
    "    print(\"result:\",res[4])\n",
    "    for j in range(0,numberOfCases):\n",
    "        #print(\"input\", text_targets_in_tr[j])\n",
    "        #print(\"y: \", np.argmax(res[3][j],axis = 1))\n",
    "\n",
    "        # convert input back to text:\n",
    "        output_str = decode_input_to_string(X_len_tr[j], X_tr[j], t_out_tr[j])\n",
    "\n",
    "        relevant_part_of_facit = np.logical_and(t_out_tr[j], t_mask_tr[j]).astype(int)\n",
    "        relevant_part_of_prediction = np.logical_and(np.argmax(res[3][j], axis=1), t_mask_tr[j]).astype(int)\n",
    "        np.set_printoptions(suppress=True)\n",
    "        #run only body of if statement, if you want to see every output, not just the ones failing. \n",
    "        if (relevant_part_of_facit-relevant_part_of_prediction).any():\n",
    "            print(\"fail! index\", j)\n",
    "            print(\"fail! target: \", t_out_tr[j])\n",
    "            print(\"y: \", np.argmax(res[3][j], axis=1))\n",
    "            print(\"fail! output: \")\n",
    "            print(\"\", 1.0/(1+np.exp(-res[3][j])))\n",
    "            print(\"input\", text_targets_in_tr[j])\n",
    "            print(\"Sentence:\",output_str)\n",
    "            print(\"relevant part of prediction \", relevant_part_of_prediction)\n",
    "            print(\"\")\n",
    "            failed = failed+1\n",
    "            #break\n",
    "\n",
    "print(\"Failed: \", (failed+0.0)/numberOfCases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Close and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
