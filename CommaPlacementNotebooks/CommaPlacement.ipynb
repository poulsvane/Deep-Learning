{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence number 6:  107252\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib nbagg\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from data_generator_tensorflow import get_batch, print_valid_characters\n",
    "from DataGenerationCommaPlacement import get_batch_comma\n",
    "from DataGenerationCommaPlacement import num_of_training_samples\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join('.', '..')) \n",
    "from tempUtils import utils \n",
    "\n",
    "import tf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# At the bottom of the script there is some code which saves the model.\n",
    "# If you wish to restore your model from a previous state use this function.\n",
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 14)\n",
      "input types: int32\n",
      "\n",
      "SAMPLE 0\n",
      "ENCODED INPUTS:\t\t\t [48887 51435 51435 74718 74718 74718 99060 37710 37710 40510 40510 40510\n",
      " 40510 47685]\n",
      "INPUTS SEQUENCE LENGTH:\t\t 14\n",
      "TEXT TARGETS OUTPUT:\t\t 6#\n",
      "TEXT TARGETS INPUT:\t\t #48887 51435 51435 74718 74718 74718 99060 37710 37710 40510 40510 40510 40510 47685 \n",
      "ENCODED TARGETS OUTPUT:\t\t [     6 125000      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0]\n",
      "ENCODED TARGETS INPUT:\t\t [48887 51435 51435 74718 74718 74718 99060 37710 37710 40510 40510 40510\n",
      " 40510 47685]\n",
      "TARGETS SEQUENCE LENGTH:\t 14\n",
      "TARGETS MASK:\t\t\t [ 1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "#inputs, inputs_seqlen, targets_in, targets_out, targets_seqlen, targets_mask, \\\n",
    "#text_inputs, text_targets_in, text_targets_out = \\\n",
    "#        get_batch(batch_size=batch_size, max_digits=10, min_digits=5)\n",
    "\n",
    "inputs, inputs_seqlen, targets_in, targets_out, targets_seqlen, targets_mask, \\\n",
    "text_targets_in, text_targets_out = \\\n",
    "        get_batch_comma(batch_size=batch_size, indices_of_interest = [0])\n",
    "    \n",
    "    \n",
    "print(inputs.shape)\n",
    "print(\"input types:\", inputs.dtype)#, inputs_seqlen.dtype, targets_in.dtype, targets_out.dtype, targets_seqlen.dtype)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    print(\"\\nSAMPLE\",i)\n",
    "    print(\"ENCODED INPUTS:\\t\\t\\t\", inputs[i])\n",
    "    print(\"INPUTS SEQUENCE LENGTH:\\t\\t\", inputs_seqlen[i])\n",
    "    print(\"TEXT TARGETS OUTPUT:\\t\\t\", text_targets_out[i])\n",
    "    print(\"TEXT TARGETS INPUT:\\t\\t\", text_targets_in[i])\n",
    "    print(\"ENCODED TARGETS OUTPUT:\\t\\t\", targets_out[i])\n",
    "    print(\"ENCODED TARGETS INPUT:\\t\\t\", targets_in[i])\n",
    "    print(\"TARGETS SEQUENCE LENGTH:\\t\", targets_seqlen[i])\n",
    "    print(\"TARGETS MASK:\\t\\t\\t\", targets_mask[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder model setup\n",
    "Below is the TensorFlow model definition. We use an embedding layer to go from integer representation to vector representation of the input.\n",
    "\n",
    "TensorFlow has implementations of LSTM and GRU units.\n",
    "Both implementations assume that the input from the tensor below has the shape **`[batch_size, max_time, input_size]`**, (unless you have `time_major=True`, in which case it is `[max_time, batch_size, input_size]`).\n",
    "\n",
    "Note that we have made use of a custom decoder wrapper which can be found in `tf_utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resetting the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Setting up hyperparameters and general configs\n",
    "MAX_DIGITS = 10\n",
    "MIN_DIGITS = 5\n",
    "NUM_INPUTS = 125002\n",
    "NUM_OUTPUTS = 125002 #(0-19 + '#'+\",\")\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "# try various learning rates 1e-2 to 1e-5\n",
    "LEARNING_RATE = 0.005\n",
    "X_EMBEDDINGS = 8\n",
    "t_EMBEDDINGS = 8\n",
    "NUM_UNITS_ENC = 16\n",
    "NUM_UNITS_DEC = 16\n",
    "\n",
    "\n",
    "# Setting up placeholders, these are the tensors that we \"feed\" to our network\n",
    "Xs = tf.placeholder(tf.int32, shape=[None, None], name='X_input')\n",
    "ts_in = tf.placeholder(tf.int32, shape=[None, None], name='t_input_in')\n",
    "ts_out = tf.placeholder(tf.int32, shape=[None, None], name='t_input_out')\n",
    "X_len = tf.placeholder(tf.int32, shape=[None], name='X_len')\n",
    "t_len = tf.placeholder(tf.int32, shape=[None], name='X_len')\n",
    "t_mask = tf.placeholder(tf.float32, shape=[None, None], name='t_mask')\n",
    "\n",
    "\n",
    "### Building the model\n",
    "# first we build the embeddings to make our characters into dense, trainable vectors\n",
    "X_embeddings = tf.get_variable('X_embeddings', [NUM_INPUTS, X_EMBEDDINGS],\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "t_embeddings = tf.get_variable('t_embeddings', [NUM_OUTPUTS, t_EMBEDDINGS],\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "X_embedded = tf.gather(X_embeddings, Xs, name='embed_X')\n",
    "t_embedded = tf.gather(t_embeddings, ts_in, name='embed_t')\n",
    "\n",
    "\n",
    "## forward encoding\n",
    "enc_cell = tf.nn.rnn_cell.GRUCell(NUM_UNITS_ENC)\n",
    "_, enc_state = tf.nn.dynamic_rnn(cell=enc_cell, inputs=X_embedded,\n",
    "                                 sequence_length=X_len, dtype=tf.float32)\n",
    "# use below incase TF's makes issues\n",
    "# enc_state, _ = tf_utils.encoder(X_embedded, X_len, 'encoder', NUM_UNITS_ENC)\n",
    "#\n",
    "# enc_state = tf.concat(1, [enc_state, enc_state])\n",
    "\n",
    "## decoding\n",
    "# note that we are using a wrapper for decoding here, this wrapper is hardcoded to only use GRU\n",
    "# check out tf_utils to see how you make your own decoder\n",
    "\n",
    "# setting up weights for computing the final output\n",
    "W_out = tf.get_variable('W_out', [NUM_UNITS_DEC, NUM_OUTPUTS])\n",
    "b_out = tf.get_variable('b_out', [NUM_OUTPUTS])\n",
    "\n",
    "dec_out, valid_dec_out = tf_utils.decoder(enc_state, t_embedded, t_len, \n",
    "                                          NUM_UNITS_DEC, t_embeddings,\n",
    "                                          W_out, b_out)\n",
    "\n",
    "## reshaping to have [batch_size*seqlen, num_units]\n",
    "out_tensor = tf.reshape(dec_out, [-1, NUM_UNITS_DEC])\n",
    "valid_out_tensor = tf.reshape(valid_dec_out, [-1, NUM_UNITS_DEC])\n",
    "# computing output\n",
    "out_tensor = tf.matmul(out_tensor, W_out) + b_out\n",
    "valid_out_tensor = tf.matmul(valid_out_tensor, W_out) + b_out\n",
    "\n",
    "## reshaping back to sequence\n",
    "# print('X_len', tf.shape(X_len)[0])\n",
    "b_size = tf.shape(X_len)[0] # use a variable we know has batch_size in [0]\n",
    "seq_len = tf.shape(t_embedded)[1] # variable we know has sequence length in [1]\n",
    "num_out = tf.constant(NUM_OUTPUTS) # casting NUM_OUTPUTS to a tensor variable\n",
    "out_shape = tf.concat([tf.expand_dims(b_size, 0),\n",
    "                      tf.expand_dims(seq_len, 0),\n",
    "                      tf.expand_dims(num_out, 0)],\n",
    "                     axis=0)\n",
    "\n",
    "out_tensor = tf.reshape(out_tensor, out_shape)\n",
    "valid_out_tensor = tf.reshape(valid_out_tensor, out_shape)\n",
    "# handling shape loss\n",
    "#out_tensor.set_shape([None, None, NUM_OUTPUTS])\n",
    "y = out_tensor\n",
    "y_valid = valid_out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_embeddings:0                           (110002, 8)\n",
      "t_embeddings:0                           (110002, 8)\n",
      "rnn/gru_cell/gates/kernel:0              (24, 32)\n",
      "rnn/gru_cell/gates/bias:0                (32,)\n",
      "rnn/gru_cell/candidate/kernel:0          (24, 16)\n",
      "rnn/gru_cell/candidate/bias:0            (16,)\n",
      "W_out:0                                  (16, 110002)\n",
      "b_out:0                                  (110002,)\n",
      "decoder/W_z_x:0                          (8, 16)\n",
      "decoder/W_z_h:0                          (16, 16)\n",
      "decoder/b_z:0                            (16,)\n",
      "decoder/W_r_x:0                          (8, 16)\n",
      "decoder/W_r_h:0                          (16, 16)\n",
      "decoder/b_r:0                            (16,)\n",
      "decoder/W_c_x:0                          (8, 16)\n",
      "decoder/W_c_h:0                          (16, 16)\n",
      "decoder/b_h:0                            (16,)\n"
     ]
    }
   ],
   "source": [
    "# print all the variable names and shapes\n",
    "for var in tf.global_variables ():\n",
    "    s = var.name + \" \"*(40-len(var.name))\n",
    "    print(s, var.value().get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the cost function, gradient clipping and accuracy\n",
    "Because the targets are categorical we use the cross entropy error.\n",
    "As the data is sequential we use the sequence to sequence cross entropy supplied in `tf_utils.py`.\n",
    "We use the Adam optimizer but you can experiment with the different optimizers implemented in [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/train/Optimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_and_acc(preds):\n",
    "    # sequence_loss_tensor is a modification of TensorFlow's own sequence_to_sequence_loss\n",
    "    # TensorFlow's seq2seq loss works with a 2D list instead of a 3D tensors\n",
    "    loss = tf_utils.sequence_loss_tensor(preds, ts_out, t_mask, NUM_OUTPUTS) # notice that we use ts_out here!\n",
    "\n",
    "    ## if you want regularization\n",
    "    #reg_scale = 0.00001\n",
    "    #regularize = tf.contrib.layers.l2_regularizer(reg_scale)\n",
    "    #params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    #reg_term = sum([regularize(param) for param in params])\n",
    "    #loss += reg_term\n",
    "    \n",
    "    ## calculate accuracy\n",
    "    argmax = tf.to_int32(tf.argmax(preds, 2))\n",
    "    correct = tf.to_float(tf.equal(argmax, ts_out)) * t_mask\n",
    "    accuracy = tf.reduce_sum(correct) / tf.reduce_sum(t_mask)\n",
    "    return loss, accuracy, argmax\n",
    "\n",
    "loss, accuracy, predictions = loss_and_acc(y)\n",
    "loss_valid, accuracy_valid, predictions_valid = loss_and_acc(y_valid)\n",
    "\n",
    "# use lobal step to keep track of our iterations\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# pick optimizer, try momentum or adadelta\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "\n",
    "# extract gradients for each variable\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "\n",
    "## add below for clipping by norm\n",
    "# gradients, variables = zip(*grads_and_vars)  # unzip list of tuples\n",
    "# clipped_gradients, global_norm = (\n",
    "#    tf.clip_by_global_norm(gradients, self.clip_norm) )\n",
    "# grads_and_vars = zip(clipped_gradients, variables)\n",
    "# apply gradients and make trainable function\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-660d60f29de5>:3: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    }
   ],
   "source": [
    "# print all the variable names and shapes\n",
    "# notice that we now have the optimizer Adam as well!\n",
    "for var in tf.all_variables():\n",
    "    s = var.name + \" \"*(40-len(var.name))\n",
    "    #print(s, var.value().get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Start the session\n",
    "# restricting memory usage, TensorFlow is greedy and will use all memory otherwise\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.35)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts, inter_op_parallelism_threads=6,\n",
    "                   intra_op_parallelism_threads=6))\n",
    "\n",
    "# Initialize parameters\n",
    "if load_model:\n",
    "    try:\n",
    "        tf.train.Saver().restore(sess, \"/tmp/model1.ckpt\")\n",
    "    except:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Model not found, new parameters initialized')\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAMPLE 0\n",
      "TEXT TARGETS INPUT:\t\t #48887 51435 51435 74718 74718 74718 99060 37710 37710 40510 40510 40510 40510 47685 \n",
      "inputs [[48887 51435 51435 74718 74718 74718 99060 37710 37710 40510 40510 40510\n",
      "  40510 47685]]\n",
      "input length [14]\n",
      "encoder state (1, 16)\n",
      "targets output [[     6 125000      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0]]\n",
      "y (1, 14, 125002)\n",
      "X_embedded (1, 14, 8)\n",
      "y_valid (1, 14, 125002)\n"
     ]
    }
   ],
   "source": [
    "# as always, test the forward pass and initialize the tf.Session!\n",
    "for i in range(batch_size):\n",
    "    print(\"\\nSAMPLE\",i)\n",
    "   # print(\"TEXT INPUTS:\\t\\t\\t\", text_inputs[i])\n",
    "    print(\"TEXT TARGETS INPUT:\\t\\t\", text_targets_in[i])\n",
    "\n",
    "feed_dict = {Xs: inputs, X_len: inputs_seqlen, ts_in: targets_in,\n",
    "             ts_out: targets_out, t_len: targets_seqlen}\n",
    "\n",
    "# test training forwardpass\n",
    "fetches = [y, X_embedded, enc_state]\n",
    "res = sess.run(fetches=fetches, feed_dict=feed_dict)\n",
    "print(\"inputs\", inputs)\n",
    "print(\"input length\", inputs_seqlen)\n",
    "print(\"encoder state\", res[2].shape)\n",
    "print(\"targets output\", targets_out)\n",
    "\n",
    "print(\"y\", res[0].shape)\n",
    "print(\"X_embedded\", res[1].shape)\n",
    "\n",
    "# test validation forwardpass\n",
    "fetches = [y_valid]\n",
    "res = sess.run(fetches=fetches, feed_dict=feed_dict)\n",
    "print(\"y_valid\", res[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Training RNN can take a while, especially if you are running it on your laptop.\n",
    "We won't train the model to completion, as the trends we are interested in can be seen earlier.\n",
    "If training takes to long feel free to stop it even earlier by interrupting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1071\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "num_of_training_samples_loaded = num_of_training_samples()\n",
    "num_of_samples_for_validation = 100\n",
    "num_of_training_samples_minus_validation = num_of_training_samples_loaded - num_of_samples_for_validation\n",
    "num_batches_train = int(num_of_training_samples_minus_validation / BATCH_SIZE)\n",
    "print(num_batches_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "X_val (100, 33)\n",
      "t_out_val (100, 33)\n"
     ]
    }
   ],
   "source": [
    "#Generate some validation data\n",
    "validation_indices = list(range(num_of_training_samples_minus_validation, num_of_training_samples_minus_validation+num_of_samples_for_validation))\n",
    "print(len(validation_indices))\n",
    "X_val, X_len_val, t_in_val, t_out_val, t_len_val, t_mask_val, \\\n",
    "text_targets_in_val, text_targets_out_val = \\\n",
    "    get_batch_comma(batch_size=num_of_samples_for_validation, indices_of_interest = validation_indices)\n",
    "print(\"X_val\", X_val.shape)\n",
    "print(\"t_out_val\", t_out_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 new Batch: 1 \n",
      "Epoch 0 new Batch: 2 \n",
      "Epoch 0 new Batch: 3 \n",
      "Epoch 0 new Batch: 4 \n",
      "Epoch 0 new Batch: 5 \n",
      "Epoch 0 new Batch: 6 \n",
      "Epoch 0 new Batch: 7 \n",
      "Epoch 0 new Batch: 8 \n",
      "Epoch 0 new Batch: 9 \n",
      "Epoch 0 new Batch: 10 \n",
      "Epoch 0 new Batch: 11 \n",
      "Epoch 0 new Batch: 12 \n",
      "Epoch 0 new Batch: 13 \n",
      "Epoch 0 new Batch: 14 \n",
      "Epoch 0 new Batch: 15 \n",
      "Epoch 0 new Batch: 16 \n",
      "Epoch 0 new Batch: 17 \n",
      "Epoch 0 new Batch: 18 \n",
      "Epoch 0 new Batch: 19 \n",
      "Epoch 0 new Batch: 20 \n",
      "Epoch 0 new Batch: 21 \n",
      "Epoch 0 new Batch: 22 \n",
      "Epoch 0 new Batch: 23 \n",
      "Epoch 0 new Batch: 24 \n",
      "Epoch 0 new Batch: 25 \n",
      "Epoch 0 new Batch: 26 \n",
      "Epoch 0 new Batch: 27 \n",
      "Epoch 0 new Batch: 28 \n",
      "Epoch 0 new Batch: 29 \n",
      "Epoch 0 new Batch: 30 \n",
      "Epoch 0 new Batch: 31 \n",
      "Epoch 0 new Batch: 32 \n",
      "Epoch 0 new Batch: 33 \n",
      "Epoch 0 new Batch: 34 \n",
      "Epoch 0 new Batch: 35 \n",
      "Epoch 0 new Batch: 36 \n",
      "Epoch 0 new Batch: 37 \n",
      "Epoch 0 new Batch: 38 \n",
      "Epoch 0 new Batch: 39 \n",
      "Epoch 0 new Batch: 40 \n",
      "Epoch 0 new Batch: 41 \n",
      "Epoch 0 new Batch: 42 \n",
      "Epoch 0 new Batch: 43 \n",
      "Epoch 0 new Batch: 44 \n",
      "Epoch 0 new Batch: 45 \n",
      "Epoch 0 new Batch: 46 \n",
      "Epoch 0 new Batch: 47 \n",
      "Epoch 0 new Batch: 48 \n",
      "Epoch 0 new Batch: 49 \n",
      "Epoch 0 new Batch: 50 \n",
      "Epoch 0 new Batch: 51 \n",
      "Epoch 0 new Batch: 52 \n",
      "Epoch 0 new Batch: 53 \n",
      "Epoch 0 new Batch: 54 \n",
      "Epoch 0 new Batch: 55 \n",
      "Epoch 0 new Batch: 56 \n",
      "Epoch 0 new Batch: 57 \n",
      "Epoch 0 new Batch: 58 \n",
      "Epoch 0 new Batch: 59 \n",
      "Epoch 0 new Batch: 60 \n",
      "Epoch 0 new Batch: 61 \n",
      "Epoch 0 new Batch: 62 \n",
      "Epoch 0 new Batch: 63 \n",
      "Epoch 0 new Batch: 64 \n",
      "Epoch 0 new Batch: 65 \n",
      "Epoch 0 new Batch: 66 \n",
      "Epoch 0 new Batch: 67 \n",
      "Epoch 0 new Batch: 68 \n",
      "Epoch 0 new Batch: 69 \n",
      "Epoch 0 new Batch: 70 \n",
      "Epoch 0 new Batch: 71 \n",
      "Epoch 0 new Batch: 72 \n",
      "Epoch 0 new Batch: 73 \n",
      "Epoch 0 new Batch: 74 \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## If you get an error, remove this line! It makes the error message hard to understand.\n",
    "\n",
    "# setting up running parameters\n",
    "val_interval = 100\n",
    "\n",
    "samples_val = []\n",
    "costs, accs_val = [], []\n",
    "plt.figure()\n",
    "batch_number = 0\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_batches = []\n",
    "        for i in range(num_batches_train): \n",
    "            # load data\n",
    "            print(\"Epoch {0} new Batch: {1} \".format(epoch, i))\n",
    "            \n",
    "            #Select random part of the training data   \n",
    "            idxs = np.random.choice(range(num_of_training_samples_minus_validation), size=(BATCH_SIZE), replace=False)\n",
    "            X_tr, X_len_tr, t_in_tr, t_out_tr, t_len_tr, t_mask_tr, \\\n",
    "            text_targets_in_tr, text_targets_out_tr = \\\n",
    "                get_batch_comma(batch_size=BATCH_SIZE, indices_of_interest = idxs)\n",
    "            # make fetches\n",
    "            fetches_tr = [train_op, loss, accuracy]\n",
    "            # set up feed dict\n",
    "            feed_dict_tr = {Xs: X_tr, X_len: X_len_tr, ts_in: t_in_tr,\n",
    "                 ts_out: t_out_tr, t_len: t_len_tr, t_mask: t_mask_tr}\n",
    "            # run the model\n",
    "            res = tuple(sess.run(fetches=fetches_tr, feed_dict=feed_dict_tr))\n",
    "            _, batch_cost, batch_acc = res\n",
    "            costs += [batch_cost]\n",
    "            \n",
    "            #if samples_processed % 1000 == 0: print(batch_cost, batch_acc)\n",
    "            #validation data\n",
    "            if i % val_interval == 0:\n",
    "                print(\"validating\")\n",
    "                fetches_val = [accuracy_valid, y_valid]\n",
    "                feed_dict_val = {Xs: X_val, X_len: X_len_val, ts_in: t_in_val,\n",
    "                 ts_out: t_out_val, t_len: t_len_val, t_mask: t_mask_val}\n",
    "                res = tuple(sess.run(fetches=fetches_val, feed_dict=feed_dict_val))\n",
    "                \n",
    "                acc_val, output_val = res\n",
    "                \n",
    "                accs_val += [acc_val]\n",
    "                epoch_batches +=[epoch+i]\n",
    "                print(\"val: \", epoch_batches)\n",
    "                print(\"accs_val: \", accs_val)\n",
    "                plt.plot(epoch_batches, accs_val, 'g-')\n",
    "                plt.ylabel('Validation Accuracy', fontsize=15)\n",
    "                plt.xlabel('Epoch+Sample', fontsize=15)\n",
    "                plt.title('', fontsize=20)\n",
    "                plt.grid('on')\n",
    "                plt.savefig(\"out.png\")\n",
    "                display.display(display.Image(filename=\"out.png\"))\n",
    "                display.clear_output(wait=True)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d162608da6bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#plot of validation accuracy for each target position\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mt_out_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Target position'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output_val' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0xebeb9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot of validation accuracy for each target position\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(np.mean(np.argmax(output_val,axis=2)==t_out_val,axis=0))\n",
    "plt.ylabel('Accuracy', fontsize=15)\n",
    "plt.xlabel('Target position', fontsize=15)\n",
    "#plt.title('', fontsize=20)\n",
    "plt.grid('on')\n",
    "plt.show()\n",
    "#why do the plot look like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: /tmp/model1.ckpt\n"
     ]
    }
   ],
   "source": [
    "## Save model\n",
    "# Read more about saving and loading models at https://www.tensorflow.org/programmers_guide/saved_model\n",
    "\n",
    "# Save model\n",
    "save_path = tf.train.Saver().save(sess, \"/tmp/model1.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ['nine five two eight nine zero eight six zero']\n",
      "y:  [[ 8 10 10 10 10 10 10 10 10 10 10]]\n"
     ]
    }
   ],
   "source": [
    "X_tr, X_len_tr, t_in_tr, t_out_tr, t_len_tr, t_mask_tr, \\\n",
    "        text_inputs_tr, text_targets_in_tr, text_targets_out_tr = \\\n",
    "            get_batch(batch_size=1,max_digits=MAX_DIGITS,min_digits=MIN_DIGITS)\n",
    "# make fetches\n",
    "fetches_tr = [train_op, loss, accuracy, y]\n",
    "# set up feed dict\n",
    "feed_dict_tr = {Xs: X_tr, X_len: X_len_tr, ts_in: t_in_tr,\n",
    "             ts_out: t_out_tr, t_len: t_len_tr, t_mask: t_mask_tr}\n",
    "# run the model\n",
    "res = tuple(sess.run(fetches=fetches_tr, feed_dict=feed_dict_tr))\n",
    "print(\"input\", text_inputs_tr)\n",
    "print(\"y: \", np.argmax(res[3],axis = 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Close the session, and free the resources\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1)\n",
    "1. What is the final validation performance? \n",
    "1. Why do you think it is not better?\n",
    "1. Comment on the accuracy for each position in of the output symbols?\n",
    "\n",
    "___\n",
    "Answer:\n",
    "1. 0.65\n",
    "1. probably because the training were cut short. I tried running it for 2e6 iterations instead, which resulted in an accuracy of approximately 0.9\n",
    "1. The accuracy of the first symbol is clearly the highest, after that the accuracy drops slightly for the ensuing symbols, and then starts to sharply plummet. This is probably because an error in the first symbols will affect the prediction for the next symbols, and make it even harder to do a correct prediction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2)\n",
    "The model has two GRU networks. The ```GRUEncoder``` and the ```GRUDecoder```.\n",
    "A GRU is parameterized by a update gate $u$, a  reset gate $r$, the cell $c$, and the hidden state $h$:\n",
    "\n",
    "![](images/GRUeq.png)\n",
    "*Equations as described in the [Lasagne GRU documentation](http://lasagne.readthedocs.io/en/latest/modules/layers/recurrent.html#lasagne.layers.GRULayer).*\n",
    "\n",
    "**Note** that the notation in the implementation (`tf_utils.py`) $z$ is used instead of $u$.\n",
    "\n",
    "Under normal circumstances, such as in the TensorFlow GRUCell implementation, these gates have been stacked for faster computation, but in the custom decoder each weight and bias are as described in the original [article for GRU](https://arxiv.org/abs/1406.1078).\n",
    " 1. Try to explain the shape of $W_{xr}$ and $W_{hr}$.\n",
    " 1. Why are they different? \n",
    "\n",
    "___\n",
    "Answer: [Suggested Answer]\n",
    "1. (1a) Lets look at $W_{xr}$: r determines the number of parameters the r gate contains. x is the size of the input vector. Thus the matrix $W_{xr}$ contains the weights for all the transitions between the input and the parameters in the r gate. So the final shape will be (number_of_input_parameters, number_of_parameters_in_r_gate).\n",
    "1. (1b) Now let us look at $W_{hr}$: h determines the size of the *state*, the number of parameters in the hidden state. r has the same meaning as before. This means that the matrix $W_{hr}$ will contain the weights for all the transitions from the prior hidden state to the parameters in the read gate. Thus the shape will be: (number_of_input_parameters, number_of_parameters_in_r_gate)\n",
    "1. In a GRU cell the external input at time t, doesn't have to have the same size as the hidden state (and hence the prior state). Thus it is necessary that x and h can vary in size, and hence also the weight matrices be different.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3)\n",
    "The GRU-unit is able to ignore the input and just copy the previous hidden state.\n",
    "In the beginning of training this might be desireable behaviour because it helps the model learn long range dependencies.\n",
    "You can make the model ignore the input by modifying initial bias values.\n",
    "1. What bias would you modify and how would you modify it?\n",
    "\n",
    "Again you'll need to refer to the [GRU equations](http://lasagne.readthedocs.io/en/latest/modules/layers/recurrent.html#lasagne.layers.GRULayer)\n",
    "Further, if you look into `tf_utils.py` and search for the `decoder(...)` function, you will see that the init for each weight and bias can be changed.\n",
    "\n",
    "___\n",
    "Answer:\n",
    "In order for the model to ignore the input, and just copy the previous hidden state, we would have to make $u_t$ be $0$ as this would make the model ignore any internally proposed updates to the state, and just copy the previous state as per the definition of $h_t$. \n",
    "\n",
    "The only value that directly affects the value of $u_t$ is $b_u$ which could be set to $-\\infty$ or a very high negative value. In the case the activation function should squash it, so that $u_t$ will be $0$, at least if the sigmoid activation function is used for $u_t$. Which it is in the provided model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4)\n",
    "In the example we stack a softmax layer on top of a Recurrent layer. In the code snippet below explain how we can do that?\n",
    "___\n",
    "Answer:\n",
    "In order to use softmax to provide us with class probabilities for the output of a single GRU unit (i.e. batch size = 1 and sequence length = 1), we will need a fully connected layer using softmax activation, between the output nodes of the GRU cell and the output classes. \n",
    "The Tensorflow API for constructing such a layer is $[batch\\_size, num\\_of\\_params]$, and if the rank of the input is greater, then it will be flattened automatically before doing the matrix multiplication. \n",
    "Since we have a batch of 16, where each have 140 GRU unit outputs (of 10 output nodes each), we need to combine these two into one batch, in order to use the tensorflow API.\n",
    "\n",
    "This is possible and doesn't change our predictions as the classfication happens *after* the recurrent layer. At this point the $140$ GRU outputs in each sequence are *independent* of each other, and thus they might be thought of, as a batch of $140 \\cdot 16$ GRU cell outputs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_input_\t (16, 140, 40)\n",
      "l_gru_\t\t (16, 140, 10)\n",
      "gru_state\t (16, 10)\n",
      "l_reshape_\t (2240, 10)\n",
      "l_softmax_\t (2240, 11)\n",
      "l_softmax_seq_\t (16, 140, 11)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "bs_, seqlen_, numinputs_ = 16, 140, 40 # Batch_size, Sequence_length, number_of_inputs\n",
    "x_pl_ = tf.placeholder(tf.float32, [bs_, seqlen_, numinputs_])\n",
    "gru_cell_ = tf.nn.rnn_cell.GRUCell(10)\n",
    "l_gru_, gru_state_ = tf.nn.dynamic_rnn(gru_cell_, x_pl_, dtype=tf.float32)\n",
    "l_reshape_ = tf.reshape(l_gru_, [-1, 10])\n",
    "\n",
    "l_softmax_ = tf.contrib.layers.fully_connected(l_reshape_, 11, activation_fn=tf.nn.softmax)\n",
    "l_softmax_seq_ = tf.reshape(l_softmax_, [bs_, seqlen_, -1])\n",
    "\n",
    "print(\"l_input_\\t\", x_pl_.get_shape())\n",
    "print(\"l_gru_\\t\\t\", l_gru_.get_shape())\n",
    "print(\"gru_state\\t\", gru_state_.get_shape())\n",
    "print(\"l_reshape_\\t\", l_reshape_.get_shape())\n",
    "print(\"l_softmax_\\t\", l_softmax_.get_shape())\n",
    "print(\"l_softmax_seq_\\t\", l_softmax_seq_.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Exercise 1)\n",
    "Why do you think the validation performance looks more \"jig-saw\" like compared to FFN and CNN models?\n",
    "\n",
    "___\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Exercise 2)\n",
    "You are interested in doing sentiment analysis on tweets, i.e classification as positive or negative. You decide read over the twitter seqeuence and use the last hidden state to do the classification. How can you modify the small network above to only output a single classification for network? Hints: look at the gru\\_state\\_ or the [tf.slice](https://www.tensorflow.org/versions/r0.10/api_docs/python/array_ops.html#slice) in the API.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Exercise 3)\n",
    "Bidirectional Encoders are usually implemented by running a forward model and  a backward model (a forward model on a reversed sequence) separately and the concatenating them before parsing them on to the next layer. To reverse the sequence try looking at [tf.reverse_sequence](https://www.tensorflow.org/versions/r0.10/api_docs/python/array_ops.html#reverse_sequence)\n",
    "\n",
    "Implement a Bidirectional encoder model.\n",
    "Here is some code to get you started:\n",
    "\n",
    "``` python\n",
    "enc_cell = tf.nn.rnn_cell.GRUCell(NUM_UNITS_ENC)\n",
    "_, enc_state = tf.nn.dynamic_rnn(cell=enc_cell, inputs=X_embedded,\n",
    "                                 sequence_length=X_len, dtype=tf.float32, scope=\"rnn_forward\")\n",
    "X_embedded_backwards = tf.reverse_sequence(X_embedded, tf.to_int64(X_len), 1)\n",
    "enc_cell_backwards = tf.nn.rnn_cell.GRUCell(NUM_UNITS_ENC)\n",
    "_, enc_state_backwards = tf.nn.dynamic_rnn(cell=enc_cell_backwards, inputs=X_embedded_backwards,\n",
    "                                 sequence_length=X_len, dtype=tf.float32, scope=\"rnn_backward\")\n",
    "\n",
    "enc_state = tf.concat(1, [enc_state, enc_state_backwards])\n",
    "```\n",
    "\n",
    "Note: you will need to double the NUM_UNITS_DEC, as it currently does not support different sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Book Question:\n",
    "\n",
    "I am answering the question regarding what is the advantage of online learning compared to normal stochastic gradient descent learning with larger mini batch sizes (like 20):\n",
    "\n",
    "There are two main differences:\n",
    "1. The quality of the gradient updates. The batch gradient averaged over the batch has a larger likelihood of approximating the true gradient for the whole training set than the one for online learning. Hence the quality of the gradient in batch training is supposedly higher. \n",
    "1. The number of weight updates. In batch training the weights are updated once per batch, while in online learning the weights are updated after each sample has been looked at.\n",
    "\n",
    "Based on the above, we can see that in general the online learning version will tend to \"jump\" a bit more, compared to the normal mini batch version,around in the solution space, this has both advantages and disadvantaged based on the expected solution space.\n",
    "\n",
    "If the solution space contains many deep local minima's (i.e. minima's that are very difficult to escape from) Online learning might be a good choice, because it will be more likely to escape from such a minima. On the other hand if there are multiple local minima's but there are not too deep and the space in general is more smooth, it would be more feasible to use the mini batch of 20 version, as this would both be able to escape the local minimas and also \"in general\" take the faster route towards the global minima."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
